{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.datasets import mnist\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n!pip install pyswarm\nfrom pyswarm import pso","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-11 07:25:06.475929: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-11 07:25:06.476080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-11 07:25:06.635899: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RPU","metadata":{}},{"cell_type":"code","source":"# Define the activation function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Define the neural network\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.weights1 = np.random.uniform(-1, 1, (input_size, hidden_size))\n        self.bias1 = np.random.uniform(-1, 1, hidden_size)\n        self.weights2 = np.random.uniform(-1, 1, (hidden_size, output_size))\n        self.bias2 = np.random.uniform(-1, 1, output_size)\n\n    def predict(self, inputs):\n        layer1_outputs = relu(np.dot(inputs, self.weights1) + self.bias1)\n        return relu(np.dot(layer1_outputs, self.weights2) + self.bias2)\n\n# Define the accuracy function\ndef accuracy(y_true, y_pred):\n    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n\n# Define the training function\ndef train(network, inputs, outputs, epochs=100):\n    best_weights1 = network.weights1\n    best_bias1 = network.bias1\n    best_weights2 = network.weights2\n    best_bias2 = network.bias2\n    best_accuracy = 0\n    random_search_count = 0\n    epoch_count = 0\n\n    while best_accuracy < 0.80:\n        temp_weights1 = best_weights1 + np.random.uniform(-0.05, 0.05, best_weights1.shape)\n        temp_bias1 = best_bias1 + np.random.uniform(-0.05, 0.05, best_bias1.shape)\n        temp_weights2 = best_weights2 + np.random.uniform(-0.05, 0.05, best_weights2.shape)\n        temp_bias2 = best_bias2 + np.random.uniform(-0.05, 0.05, best_bias2.shape)\n\n        network.weights1 = temp_weights1\n        network.bias1 = temp_bias1\n        network.weights2 = temp_weights2\n        network.bias2 = temp_bias2\n\n        predictions = network.predict(inputs)\n        current_accuracy = accuracy(outputs, predictions)\n\n        random_search_count += 1\n        print(f'random search {random_search_count} accuracy: {current_accuracy}')\n\n        if current_accuracy > best_accuracy:\n            best_accuracy = current_accuracy\n            best_weights1 = temp_weights1\n            best_bias1 = temp_bias1\n            best_weights2 = temp_weights2\n            best_bias2 = temp_bias2\n            epoch_count += 1\n            print('------------------------------')\n            print(f'Epoch {epoch_count}/{epochs}, Accuracy: {current_accuracy}')\n            print('------------------------------')\n        else:\n            network.weights1 = best_weights1\n            network.bias1 = best_bias1\n            network.weights2 = best_weights2\n            network.bias2 = best_bias2\n\n# Load the MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nx_train = x_train.reshape(x_train.shape[0], -1) / 255\nx_test = x_test.reshape(x_test.shape[0], -1) / 255\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# Create and train the neural network\nnetwork = NeuralNetwork(x_train.shape[1], 64, y_train.shape[1])\ntrain(network, x_train, y_train)\n\n# Evaluate the network\npredictions = network.predict(x_test)\nprint(f'Test Accuracy: {accuracy(y_test, predictions)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the activation function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Define the neural network\nclass NeuralNetwork:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.weights1 = np.load(\"/kaggle/working/weights1.npy\")\n        self.bias1 = np.load(\"/kaggle/working/bias1.npy\")\n        self.weights2 = np.load(\"/kaggle/working/weights2.npy\")\n        self.bias2 = np.load(\"/kaggle/working/bias2.npy\")\n\n    def predict(self, inputs):\n        layer1_outputs = relu(np.dot(inputs, self.weights1) + self.bias1)\n        return relu(np.dot(layer1_outputs, self.weights2) + self.bias2)\n\n# Define the accuracy function\ndef accuracy(y_true, y_pred):\n    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n\n# Define the training function\ndef train(network, inputs, outputs, epochs=100):\n    best_weights1 = network.weights1\n    best_bias1 = network.bias1\n    best_weights2 = network.weights2\n    best_bias2 = network.bias2\n    best_accuracy = 0\n    random_search_count = 0\n    epoch_count = 0\n\n    while best_accuracy < 0.80:\n        temp_weights1 = best_weights1 + np.random.uniform(-0.5, 0.5, best_weights1.shape)\n        temp_bias1 = best_bias1 + np.random.uniform(-0.5, 0.5, best_bias1.shape)\n        temp_weights2 = best_weights2 + np.random.uniform(-0.5, 0.5, best_weights2.shape)\n        temp_bias2 = best_bias2 + np.random.uniform(-0.5, 0.5, best_bias2.shape)\n\n        network.weights1 = temp_weights1\n        network.bias1 = temp_bias1\n        network.weights2 = temp_weights2\n        network.bias2 = temp_bias2\n\n        predictions = network.predict(inputs)\n        current_accuracy = accuracy(outputs, predictions)\n\n        random_search_count += 1\n        print(f'random search {random_search_count} accuracy: {current_accuracy}')\n\n        if current_accuracy > best_accuracy:\n            best_accuracy = current_accuracy\n            best_weights1 = temp_weights1\n            best_bias1 = temp_bias1\n            best_weights2 = temp_weights2\n            best_bias2 = temp_bias2\n            epoch_count += 1\n            print('------------------------------')\n            print(f'Epoch {epoch_count}/{epochs}, Accuracy: {current_accuracy}')\n            print('------------------------------')\n        else:\n            network.weights1 = best_weights1\n            network.bias1 = best_bias1\n            network.weights2 = best_weights2\n            network.bias2 = best_bias2\n\n# Load the MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nx_train = x_train.reshape(x_train.shape[0], -1) / 255\nx_test = x_test.reshape(x_test.shape[0], -1) / 255\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# Create and train the neural network\nnetwork1 = NeuralNetwork(x_train.shape[1], 64, y_train.shape[1])\ntrain(network1, x_train, y_train)\n\n# Evaluate the network\npredictions = network1.predict(x_test)\nprint(f'Test Accuracy: {accuracy(y_test, predictions)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Particle Swarm Optimization","metadata":{}},{"cell_type":"code","source":"# Load MNIST dataset\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nX = X / 255.0\ny = y.astype(int)\n\n# Split dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the number of neurons in the layers\nn_input = 784\nn_hidden = 64\nn_output = 10\n\n# Activation function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Output (softmax) function\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum(axis=0)\n\n# Define the neural network\ndef neural_network(x, w):\n    w1 = w[:n_input*n_hidden].reshape((n_input, n_hidden))\n    w2 = w[n_input*n_hidden:].reshape((n_hidden, n_output))\n    z1 = np.dot(x, w1)  # Hidden layer input\n    a1 = relu(z1)  # Hidden layer output\n    z2 = np.dot(a1, w2)  # Output layer input\n    a2 = softmax(z2)  # Output layer results\n    return a2\n\n\n# Define the accuracy function\ndef accuracy(w):\n    correct = 0\n    for i in range(len(X_test)):\n        y_pred = np.argmax(neural_network(X_test.iloc[i], w))\n        if y_pred == y_test.iloc[i]:\n            correct += 1\n    return correct / len(X_test)\n\n\n\n# Define the loss function (cross-entropy)\ndef loss_function(w):\n    loss = 0\n    for i in range(len(X_train)):\n        y_pred = neural_network(X_train.iloc[i], w)  # Use .iloc here\n        target = np.zeros(n_output)\n        target[y_train.iloc[i]] = 1  # And here\n        loss += -np.sum(target * np.log(y_pred))  # Cross-entropy loss\n    acc = accuracy(w)\n    print(f'Loss: {loss}, Accuracy: {acc}')\n    return loss\n\n\n\n# Use Particle Swarm Optimization to minimize the loss function\nw_opt, loss_opt = pso(loss_function, lb=-1*np.ones((n_input+n_output)*n_hidden), ub=np.ones((n_input+n_output)*n_hidden))\n\nprint(\"Optimal weights:\", w_opt)\nprint(\"Optimal loss:\", loss_opt)","metadata":{"execution":{"iopub.status.busy":"2024-04-11T07:35:36.142321Z","iopub.execute_input":"2024-04-11T07:35:36.142755Z","iopub.status.idle":"2024-04-11T07:52:20.477538Z","shell.execute_reply.started":"2024-04-11T07:35:36.142726Z","shell.execute_reply":"2024-04-11T07:52:20.475369Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n  warn(\n","output_type":"stream"},{"name":"stdout","text":"Loss: 1499776.8525151468, Accuracy: 0.081\nLoss: 1457217.4268785822, Accuracy: 0.05878571428571429\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 60\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Use Particle Swarm Optimization to minimize the loss function\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m w_opt, loss_opt \u001b[38;5;241m=\u001b[39m \u001b[43mpso\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_input\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn_output\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mub\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_input\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mn_output\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mn_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal weights:\u001b[39m\u001b[38;5;124m\"\u001b[39m, w_opt)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss_opt)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyswarm/pso.py:111\u001b[0m, in \u001b[0;36mpso\u001b[0;34m(func, lb, ub, ieqcons, f_ieqcons, args, kwargs, swarmsize, omega, phip, phig, maxiter, minstep, minfunc, debug)\u001b[0m\n\u001b[1;32m    108\u001b[0m p[i, :] \u001b[38;5;241m=\u001b[39m x[i, :]\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Calculate the objective's value at the current particle's\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m fp[i] \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# At the start, there may not be any feasible starting point, so just\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# give it a temporary \"best\" point since it's likely to change\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyswarm/pso.py:74\u001b[0m, in \u001b[0;36mpso.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     71\u001b[0m vlow \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mvhigh\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Check for constraint function(s) #########################################\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f_ieqcons \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ieqcons):\n","Cell \u001b[0;32mIn[13], line 49\u001b[0m, in \u001b[0;36mloss_function\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)):\n\u001b[0;32m---> 49\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m neural_network(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, w)  \u001b[38;5;66;03m# Use .iloc here\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n_output)\n\u001b[1;32m     51\u001b[0m     target[y_train\u001b[38;5;241m.\u001b[39miloc[i]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# And here\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexing.py:1754\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:3984\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3982\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3984\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3986\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3987\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/internals/managers.py:1001\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    996\u001b[0m     result \u001b[38;5;241m=\u001b[39m ensure_wrapped_if_datetimelike(result)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Such assignment may incorrectly coerce NaT to None\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, rl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1002\u001b[0m         result[rl] \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39miget((i, loc))\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}